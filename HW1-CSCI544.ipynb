{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\saira\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Invalid requirement: '#'\n"
     ]
    }
   ],
   "source": [
    "! pip install bs4 # in case you don't have it installed\n",
    "\n",
    "# Dataset: https://web.archive.org/web/20201127142707if_/https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Office_Products_v1_00.tsv.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saira\\AppData\\Local\\Temp\\ipykernel_31388\\2628411588.py:1: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('D:/Studies/NLP/NLPHW1/dataset/amazon_reviews_us_Office_Products_v1_00.tsv', sep='\\t',  on_bad_lines='skip')\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('D:/Studies/NLP/NLPHW1/dataset/amazon_reviews_us_Office_Products_v1_00.tsv', sep='\\t',  on_bad_lines='skip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keep Reviews and Ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_df = df[[\"star_rating\", \"review_body\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## We form two classes and select 50000 reviews randomly from each class.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saira\\AppData\\Local\\Temp\\ipykernel_31388\\3200697383.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  target_df[\"Classes\"] = class_list\n"
     ]
    }
   ],
   "source": [
    "class_list = []\n",
    "neg = [1, 2, 3] #class1\n",
    "pos = [4, 5] #class2\n",
    "for i in target_df[\"star_rating\"]:\n",
    "    if (i == 5 or i == 4):\n",
    "        class_list.append(\"pos\")\n",
    "    elif (i == 1 or i == 2 or i ==3):\n",
    "        class_list.append(\"neg\")\n",
    "    else:\n",
    "        try:\n",
    "            if (int(i) == 5 or int(i) == 4):\n",
    "                class_list.append(\"pos\")\n",
    "            elif (int(i) == 1 or int(i) == 2 or int(i) ==3):\n",
    "                class_list.append(\"neg\")\n",
    "        except:\n",
    "            class_list.append(\"XO\")\n",
    "target_df[\"Classes\"] = class_list\n",
    "rmd = [\"XO\"]\n",
    "target_df = target_df[~target_df['Classes'].isin(rmd)]\n",
    "neg_df = target_df.query('(Classes != \"pos\")').sample(n=50000)\n",
    "pos_df = target_df.query('(Classes != \"neg\")').sample(n=50000)\n",
    "frames = [neg_df, pos_df]\n",
    "result = pd.concat(frames)\n",
    "final_df = result.sample(frac=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning & Pre-Processing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df[\"review_body\"] = final_df[\"review_body\"].str.lower()\n",
    "test = final_df\n",
    "for i in range(len(test)):\n",
    "    test.at[test.index[i],'review_body'] = re.sub(r'[^a-zA-Z0-9\" \"]', \"\", str(test.at[test.index[i],'review_body']))# removing no alphanumeric\n",
    "    test.at[test.index[i],'review_body'] = re.sub(r'''(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’]))''', \" \", str(test.at[test.index[i],'review_body'])) #removing URLS\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## remove the stop words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize #stopwords\n",
    "from nltk.corpus import stopwords\n",
    "stops = set(stopwords.words('english'))\n",
    "teest = final_df\n",
    "for i in range(len(teest)):\n",
    "    words = word_tokenize(str(teest.loc[teest.index[i],'review_body']))\n",
    "    wordsFiltered = []\n",
    "    for w in words:\n",
    "        if w not in stops:\n",
    "            wordsFiltered.append(w)\n",
    "    teest.at[teest.index[i],'review_body'] = ' '.join(wordsFiltered)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## perform lemmatization  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize #lemmatization\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "teeest = final_df\n",
    "for i in range(len(teeest)):\n",
    "    words = word_tokenize(str(teeest.loc[teeest.index[i],'review_body']))\n",
    "    newwords=[]\n",
    "    for w in words:\n",
    "        newwords.append(wnl.lemmatize(w))\n",
    "    teeest.at[teeest.index[i],'review_body'] = ' '.join(newwords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df['label'] = final_df['Classes'].map({'pos':1, 'neg':0})\n",
    "X = final_df.review_body\n",
    "Y = final_df.label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF and BoW Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "data1 = vectorizer.fit_transform(X)\n",
    "# print(vectorizer.get_feature_names_out())\n",
    "# print(vectorizer.vocabulary_)\n",
    "# print()\n",
    "bow_vectorizer = CountVectorizer()\n",
    "data2 = bow_vectorizer.fit_transform(X)\n",
    "# print(bow_vectorizer.get_feature_names_out())\n",
    "# print(bow_vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "#train test split for tfidf\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(data1, Y, test_size=0.2, random_state=2)\n",
    "#train test split for bow\n",
    "x1train, x1test, y1train, y1test = train_test_split(data2, Y, test_size=0.2, random_state=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron Using Both Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Accuracy of the tfidf classifier is 0.7956\n",
      "\n",
      " Confusion matrix\n",
      "[[7949 1965]\n",
      " [2123 7963]]\n",
      "\n",
      " The value of Precision 0.8020749395648671\n",
      "\n",
      " The value of Recall 0.7895102121752925\n",
      "\n",
      "F1 Score:  0.7957429799140602\n",
      "\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.80      0.80      9914\n",
      "           1       0.80      0.79      0.80     10086\n",
      "\n",
      "    accuracy                           0.80     20000\n",
      "   macro avg       0.80      0.80      0.80     20000\n",
      "weighted avg       0.80      0.80      0.80     20000\n",
      "\n",
      "\n",
      "*****************************************************\n",
      "\n",
      "\n",
      " Accuracy of the bow classifier is 0.796\n",
      "\n",
      " Confusion matrix\n",
      "[[7887 2027]\n",
      " [2053 8033]]\n",
      "\n",
      " The value of Precision 0.7985089463220676\n",
      "\n",
      " The value of Recall 0.7964505254808646\n",
      "\n",
      "F1 Score:  0.7974784076243423\n",
      "\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.80      0.79      9914\n",
      "           1       0.80      0.80      0.80     10086\n",
      "\n",
      "    accuracy                           0.80     20000\n",
      "   macro avg       0.80      0.80      0.80     20000\n",
      "weighted avg       0.80      0.80      0.80     20000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Perceptron \n",
    "#tfidf Perceptron\n",
    "tf_pc = Perceptron(random_state=42, max_iter=2500).fit(xtrain, ytrain)\n",
    "tf_pc_predict = tf_pc.predict(xtest)\n",
    "print('\\n Accuracy of the tfidf classifier is',metrics.accuracy_score(ytest,tf_pc_predict))  #tfidf Perceptron\n",
    "print('\\n Confusion matrix')\n",
    "print(metrics.confusion_matrix(ytest,tf_pc_predict))\n",
    "print('\\n The value of Precision', metrics.precision_score(ytest,tf_pc_predict))\n",
    "print('\\n The value of Recall', metrics.recall_score(ytest,tf_pc_predict))\n",
    "print(\"\\nF1 Score: \",metrics.f1_score(ytest, tf_pc_predict))\n",
    "print(\"\\nClassification Report: \\n\",metrics.classification_report(ytest, tf_pc_predict))\n",
    "print(\"\\n*****************************************************\\n\")\n",
    "#BOW Perceptron\n",
    "bow_pc = Perceptron(random_state=42, max_iter=2500).fit(x1train, y1train)\n",
    "bow_pc_predict = bow_pc.predict(x1test)\n",
    "print('\\n Accuracy of the bow classifier is',metrics.accuracy_score(y1test,bow_pc_predict)) #BOW Perceptron\n",
    "print('\\n Confusion matrix')\n",
    "print(metrics.confusion_matrix(y1test,bow_pc_predict))\n",
    "print('\\n The value of Precision', metrics.precision_score(y1test,bow_pc_predict))\n",
    "print('\\n The value of Recall', metrics.recall_score(y1test,bow_pc_predict))\n",
    "print(\"\\nF1 Score: \",metrics.f1_score(y1test, bow_pc_predict))\n",
    "print(\"\\nClassification Report: \\n\",metrics.classification_report(y1test, bow_pc_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Using Both Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Accuracy of the tfidf classifier is 0.8276\n",
      "\n",
      " Confusion matrix\n",
      "[[8223 1691]\n",
      " [1757 8329]]\n",
      "\n",
      " The value of Precision 0.8312375249500998\n",
      "\n",
      " The value of Recall 0.8257981360301407\n",
      "\n",
      "F1 Score:  0.8285089028150802\n",
      "\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.83      0.83      9914\n",
      "           1       0.83      0.83      0.83     10086\n",
      "\n",
      "    accuracy                           0.83     20000\n",
      "   macro avg       0.83      0.83      0.83     20000\n",
      "weighted avg       0.83      0.83      0.83     20000\n",
      "\n",
      "\n",
      "*****************************************************\n",
      "\n",
      "\n",
      " Accuracy of the bow classifier is 0.8453\n",
      "\n",
      " Confusion matrix\n",
      "[[8186 1728]\n",
      " [1366 8720]]\n",
      "\n",
      " The value of Precision 0.8346094946401225\n",
      "\n",
      " The value of Recall 0.8645647432084077\n",
      "\n",
      "F1 Score:  0.8493230739261712\n",
      "\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.83      0.84      9914\n",
      "           1       0.83      0.86      0.85     10086\n",
      "\n",
      "    accuracy                           0.85     20000\n",
      "   macro avg       0.85      0.85      0.85     20000\n",
      "weighted avg       0.85      0.85      0.85     20000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression \n",
    "#tfidf logstic regression\n",
    "lr = LogisticRegression(C=100, random_state=1, solver='saga', multi_class='auto', max_iter=5000).fit(xtrain, ytrain)\n",
    "tf_predict = lr.predict(xtest)\n",
    "print('\\n Accuracy of the tfidf classifier is',metrics.accuracy_score(ytest,tf_predict))  #tfidf logstic regression\n",
    "print('\\n Confusion matrix')\n",
    "print(metrics.confusion_matrix(ytest,tf_predict))\n",
    "print('\\n The value of Precision', metrics.precision_score(ytest,tf_predict))\n",
    "print('\\n The value of Recall', metrics.recall_score(ytest,tf_predict))\n",
    "print(\"\\nF1 Score: \",metrics.f1_score(ytest, tf_predict))\n",
    "print(\"\\nClassification Report: \\n\",metrics.classification_report(ytest, tf_predict))\n",
    "print(\"\\n*****************************************************\\n\")\n",
    "#bow logstic regression\n",
    "lr1 = LogisticRegression(C=100, random_state=1, solver='saga', multi_class='auto', max_iter=4000).fit(x1train, y1train)\n",
    "bow_predict = lr1.predict(x1test)\n",
    "print('\\n Accuracy of the bow classifier is',metrics.accuracy_score(y1test,bow_predict))  #bow logstic regression\n",
    "print('\\n Confusion matrix')\n",
    "print(metrics.confusion_matrix(y1test,bow_predict))\n",
    "print('\\n The value of Precision', metrics.precision_score(y1test,bow_predict))\n",
    "print('\\n The value of Recall', metrics.recall_score(y1test,bow_predict))\n",
    "print(\"\\nF1 Score: \",metrics.f1_score(y1test, bow_predict))\n",
    "print(\"\\nClassification Report: \\n\",metrics.classification_report(y1test, bow_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Using Both Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Accuracy of the tfidf classifier is 0.81935\n",
      "\n",
      " Confusion matrix\n",
      "[[8477 1437]\n",
      " [2176 7910]]\n",
      "\n",
      " The value of Precision 0.8462608323526265\n",
      "\n",
      " The value of Recall 0.7842554035296451\n",
      "\n",
      "F1 Score:  0.8140791437245921\n",
      "\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.86      0.82      9914\n",
      "           1       0.85      0.78      0.81     10086\n",
      "\n",
      "    accuracy                           0.82     20000\n",
      "   macro avg       0.82      0.82      0.82     20000\n",
      "weighted avg       0.82      0.82      0.82     20000\n",
      "\n",
      "\n",
      "*****************************************************\n",
      "\n",
      "\n",
      " Accuracy of the bow classifier is 0.80985\n",
      "\n",
      " Confusion matrix\n",
      "[[7622 2292]\n",
      " [1511 8575]]\n",
      "\n",
      " The value of Precision 0.7890862243489464\n",
      "\n",
      " The value of Recall 0.8501883799325798\n",
      "\n",
      "F1 Score:  0.8184985443611893\n",
      "\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.77      0.80      9914\n",
      "           1       0.79      0.85      0.82     10086\n",
      "\n",
      "    accuracy                           0.81     20000\n",
      "   macro avg       0.81      0.81      0.81     20000\n",
      "weighted avg       0.81      0.81      0.81     20000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "#Bayesian TFIDF\n",
    "NBmodel = MultinomialNB().fit(xtrain, ytrain) \n",
    "predicted = NBmodel.predict(xtest)\n",
    "print('\\n Accuracy of the tfidf classifier is',metrics.accuracy_score(ytest,predicted))\n",
    "print('\\n Confusion matrix')\n",
    "print(metrics.confusion_matrix(ytest,predicted))\n",
    "print('\\n The value of Precision', metrics.precision_score(ytest,predicted))\n",
    "print('\\n The value of Recall', metrics.recall_score(ytest,predicted))\n",
    "print(\"\\nF1 Score: \",metrics.f1_score(ytest, predicted))\n",
    "print(\"\\nClassification Report: \\n\",metrics.classification_report(ytest, predicted))\n",
    "print(\"\\n*****************************************************\\n\")\n",
    "#Bayesian with BOW\n",
    "NBmodel1 = MultinomialNB().fit(x1train, y1train)\n",
    "predicted1 = NBmodel1.predict(x1test)\n",
    "print('\\n Accuracy of the bow classifier is',metrics.accuracy_score(y1test,predicted1))\n",
    "print('\\n Confusion matrix')\n",
    "print(metrics.confusion_matrix(y1test,predicted1))\n",
    "print('\\n The value of Precision', metrics.precision_score(y1test,predicted1))\n",
    "print('\\n The value of Recall', metrics.recall_score(y1test,predicted1))\n",
    "print(\"\\nF1 Score: \",metrics.f1_score(y1test, predicted1))\n",
    "print(\"\\nClassification Report: \\n\",metrics.classification_report(y1test, predicted1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM Using Both Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Accuracy of the tfidf classifier is 0.84785\n",
      "\n",
      " Confusion matrix\n",
      "[[8479 1435]\n",
      " [1608 8478]]\n",
      "\n",
      " The value of Precision 0.8552405931604963\n",
      "\n",
      " The value of Recall 0.8405710886377157\n",
      "\n",
      "F1 Score:  0.8478423921196059\n",
      "\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.86      0.85      9914\n",
      "           1       0.86      0.84      0.85     10086\n",
      "\n",
      "    accuracy                           0.85     20000\n",
      "   macro avg       0.85      0.85      0.85     20000\n",
      "weighted avg       0.85      0.85      0.85     20000\n",
      "\n",
      "\n",
      "*****************************************************\n",
      "\n",
      "\n",
      " Accuracy of the bow classifier is 0.83375\n",
      "\n",
      " Confusion matrix\n",
      "[[8024 1890]\n",
      " [1435 8651]]\n",
      "\n",
      " The value of Precision 0.8207001233279575\n",
      "\n",
      " The value of Recall 0.8577235772357723\n",
      "\n",
      "F1 Score:  0.8388035099626703\n",
      "\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.81      0.83      9914\n",
      "           1       0.82      0.86      0.84     10086\n",
      "\n",
      "    accuracy                           0.83     20000\n",
      "   macro avg       0.83      0.83      0.83     20000\n",
      "weighted avg       0.83      0.83      0.83     20000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "#tfidf svm\n",
    "tf_svm = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto').fit(xtrain, ytrain)\n",
    "tf_svm_predict = tf_svm.predict(xtest)\n",
    "print('\\n Accuracy of the tfidf classifier is',metrics.accuracy_score(ytest,tf_svm_predict))  #tfidf Perceptron\n",
    "print('\\n Confusion matrix')\n",
    "print(metrics.confusion_matrix(ytest,tf_svm_predict))\n",
    "print('\\n The value of Precision', metrics.precision_score(ytest,tf_svm_predict))\n",
    "print('\\n The value of Recall', metrics.recall_score(ytest,tf_svm_predict))\n",
    "print(\"\\nF1 Score: \",metrics.f1_score(ytest, tf_svm_predict))\n",
    "print(\"\\nClassification Report: \\n\",metrics.classification_report(ytest, tf_svm_predict))\n",
    "print(\"\\n*****************************************************\\n\")\n",
    "#bow svm\n",
    "bow_svm = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto').fit(x1train, y1train)\n",
    "bow_svm_predict = bow_svm.predict(x1test)\n",
    "print('\\n Accuracy of the bow classifier is',metrics.accuracy_score(y1test,bow_svm_predict)) #BOW Perceptron\n",
    "print('\\n Confusion matrix')\n",
    "print(metrics.confusion_matrix(y1test,bow_svm_predict))\n",
    "print('\\n The value of Precision', metrics.precision_score(y1test,bow_svm_predict))\n",
    "print('\\n The value of Recall', metrics.recall_score(y1test,bow_svm_predict))\n",
    "print(\"\\nF1 Score: \",metrics.f1_score(y1test, bow_svm_predict))\n",
    "print(\"\\nClassification Report: \\n\",metrics.classification_report(y1test, bow_svm_predict))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
